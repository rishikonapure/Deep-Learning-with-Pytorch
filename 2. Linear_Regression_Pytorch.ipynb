{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to implement the liner regression model using pytorch."
      ],
      "metadata": {
        "id": "vuYdHMDd8nS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "foL5yK4e9CW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16AagX1k8TJf"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Data"
      ],
      "metadata": {
        "id": "Qn5F3fo98_cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating synthetic data \n",
        "\n",
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
        "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
        "    y = torch.matmul(X, w) + b\n",
        "    y += torch.normal(0, 0.01, y.shape)\n",
        "    return X, y.reshape((-1, 1))\n",
        "\n",
        "true_w = torch.tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)"
      ],
      "metadata": {
        "id": "cTc7QWeh851g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('features:', features[0],'\\nlabel:', labels[0])"
      ],
      "metadata": {
        "id": "VUlfWGzU9H4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b85afd9-f5c8-4ce0-f805-e457f9d32af7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features: tensor([ 0.7717, -0.0837]) \n",
            "label: tensor([6.0404])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the Dataset :\n",
        "\n",
        "- Rather than rolling our own iterator, we can call upon the existing API in a framework to read data. We pass in `features` and `labels` as arguments and specify `batch_size` when instantiating a data iterator object. \n",
        "\n",
        "- boolean value `is_train` indicates whether or not we want the data iterator object to shuffle the data on each epoch.\n"
      ],
      "metadata": {
        "id": "WNbZ9Qpz92oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
        "    dataset = data.TensorDataset(*data_arrays)\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "batch_size = 10\n",
        "data_iter = load_array((features, labels), batch_size)"
      ],
      "metadata": {
        "id": "cTRMTP_z9dYY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use `data_iter` in much the same way as we called the `data_iter` function in scratch implementation.\n",
        "\n",
        "To verify that it is working, we can read and print the first minibatch of examples. \n",
        "\n",
        "Here we use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator."
      ],
      "metadata": {
        "id": "Avush484--lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(data_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9qsRy-Q9lJC",
        "outputId": "c2413e5c-9871-46f2-d7ae-813f186d91c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 1.3855,  0.6026],\n",
              "         [-1.1590, -1.1072],\n",
              "         [ 0.3469, -1.0460],\n",
              "         [-0.4812, -0.7850],\n",
              "         [-1.3633, -0.2921],\n",
              "         [ 0.4584,  0.4201],\n",
              "         [ 0.2525, -0.7704],\n",
              "         [ 0.8398,  1.3273],\n",
              "         [ 0.0024,  1.7252],\n",
              "         [ 0.9644, -0.5296]]), tensor([[ 4.9029],\n",
              "         [ 5.6415],\n",
              "         [ 8.4601],\n",
              "         [ 5.9251],\n",
              "         [ 2.4735],\n",
              "         [ 3.6923],\n",
              "         [ 7.3126],\n",
              "         [ 1.3502],\n",
              "         [-1.6613],\n",
              "         [ 7.9276]])]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model :\n",
        "\n",
        "When we implemented linear regression from scratch, we defined our model parameters explicitly and coded up the calculations to produce output using basic linear algebra operations. \n",
        "\n",
        "You should know how to do this. But once your models get more complex, and once you have to do this nearly every day, you will be glad for the assistance.\n",
        "\n",
        "`Blog Examaple`\n",
        "\n",
        "For standard operations, we can use a framework’s predefined layers, which allow us to focus especially on the layers used to construct the model rather than having to focus on the implementation.\n",
        "\n"
      ],
      "metadata": {
        "id": "5pCrX0_o_idK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will first define a model variable `net`, which will refer to an instance of the Sequential class.\n",
        "\n",
        "- Given input data, a Sequential instance passes it through the first layer, in turn passing the output as the second layer’s input and so forth."
      ],
      "metadata": {
        "id": "wJz6SJtRAxNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `nn` -- > neural networks\n",
        "from torch import nn\n",
        "\n",
        "net = nn.Sequential(nn.Linear(2, 1))"
      ],
      "metadata": {
        "id": "KWLQ5ph9_f4s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, the fully-connected layer is defined in the Linear class. Note that we passed two arguments into `nn.Linear`. The first one specifies the input feature dimension, which is 2, and the second one is the output feature dimension, which is a single scalar and therefore 1."
      ],
      "metadata": {
        "id": "7qyYCsIBAtS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inintializing model parameter : \n",
        "\n",
        "Before using net, we need to initialize the model parameters, such as the weights and bias in the linear regression model. Deep learning frameworks often have a predefined way to initialize the parameters. Here we specify that each weight parameter should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01. The bias parameter will be initialized to zero."
      ],
      "metadata": {
        "id": "a4K17yaOBuE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have specified the input and output dimensions when constructing `nn.Linear`, now we can access the parameters directly to specify their initial values.\n",
        "\n",
        "- We first locate the layer by `net[0]`, which is the first layer in the network\n",
        "- and then use the `weight.data` and `bias.data` methods to access the parameters\n",
        "- Next we use the replace methods `normal_` and `fill_` to overwrite parameter values."
      ],
      "metadata": {
        "id": "dY-Hk5LXCL3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net[0].weight.data.normal_(0, 0.01)\n",
        "net[0].bias.data.fill_(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5EwKP_hBsm2",
        "outputId": "5bb3a02b-4dac-4b40-e514-8c1329fc9eb0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the loss function \n",
        "\n",
        "The MSELoss class computes the mean squared error. By default it returns the average loss over examples."
      ],
      "metadata": {
        "id": "tf8LwA33CjXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.MSELoss()"
      ],
      "metadata": {
        "id": "HtHuy0_qCgyC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Optimization Algorithm : \n",
        "\n",
        "Minibatch stochastic gradient descent is a standard tool for optimizing neural networks and thus PyTorch supports it alongside a number of variations on this algorithm in the `optim` module.\n",
        "\n",
        "When we instantiate an SGD instance, we will specify the parameters to optimize over (obtainable from our net via `net.parameters()`), with a dictionary of hyperparameters required by our optimization algorithm.\n",
        "\n",
        "Minibatch stochastic gradient descent just requires that we set the value `lr`, which is set to 0.03 here."
      ],
      "metadata": {
        "id": "4U3bXEDGCxV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
      ],
      "metadata": {
        "id": "Z2aFqo2_Cvhm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training : \n",
        "\n",
        "You might have noticed that expressing our model through high-level APIs of a deep learning framework requires comparatively few lines of code. We did not have to individually allocate parameters, define our loss function, or implement minibatch stochastic gradient descent. Once we start working with much more complex models, advantages of high-level APIs will grow considerably. However, once we have all the basic pieces in place, the training loop itself is strikingly similar to what we did when implementing everything from scratch.\n",
        "\n",
        "To refresh your memory: for some number of epochs, we will make a complete pass over the dataset (train_data), iteratively grabbing one minibatch of inputs and the corresponding ground-truth labels. For each minibatch, we go through the following ritual:\n",
        "\n",
        "- Generate predictions by calling net(X) and calculate the loss l (the forward propagation).\n",
        "\n",
        "- Calculate gradients by running the backpropagation.\n",
        "\n",
        "- Update the model parameters by invoking our optimizer.\n",
        "\n",
        "For good measure, we compute the loss after each epoch and print it to monitor progress."
      ],
      "metadata": {
        "id": "Edll5iNcDj01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter:\n",
        "        l = loss(net(X) ,y)\n",
        "        trainer.zero_grad()\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "    l = loss(net(features), labels)\n",
        "    print(f'epoch {epoch + 1}, loss {l:f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUqPf4WSDZz3",
        "outputId": "fc2c8696-cc52-4754-89c0-7a6a81047d51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss 0.000234\n",
            "epoch 2, loss 0.000110\n",
            "epoch 3, loss 0.000108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset. To access parameters, we first access the layer that we need from net and then access that layer’s weights and bias."
      ],
      "metadata": {
        "id": "XkCuphMZEvsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = net[0].weight.data\n",
        "print('error in estimating w:', true_w - w.reshape(true_w.shape))\n",
        "b = net[0].bias.data\n",
        "print('error in estimating b:', true_b - b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TxtX1tBEwBi",
        "outputId": "328b7e4a-9825-431e-e624-05d8be701c52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error in estimating w: tensor([-0.0011, -0.0005])\n",
            "error in estimating b: tensor([0.0002])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using PyTorch’s high-level APIs, we can implement models much more concisely.\n",
        "\n",
        "- In PyTorch, the data module provides tools for data processing, the nn module defines a large number of neural network layers and common loss functions.\n",
        "\n",
        "- We can initialize the parameters by replacing their values with methods ending with _."
      ],
      "metadata": {
        "id": "0lIdTW0kFSog"
      }
    }
  ]
}